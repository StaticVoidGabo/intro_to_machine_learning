{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesamiento del lenguaje natural\n",
    "\n",
    "Tenemos un dataset que tiene textos que queremos evaluar si son tóxicos o no tóxicos. Este dataset puedes bajarlo del siguiente enlace \n",
    "[aquí](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge). \n",
    "\n",
    "Vamos a usar dos algoritmos para aprender cuando un texto es tóxico o no:\n",
    "\n",
    "\n",
    "1) Regresión logística\n",
    "\n",
    "2) Tf-idf\n",
    "\n",
    "\n",
    "Para usar ambas, primero realizaremos un proceso de tokenización y normalización del texto y luego un bag of words.\n",
    "\n",
    "\n",
    "Usaremos librerías de  `sklearn` para ello."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Leyendo los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "comments_df = pd.read_csv(\"data/jigsaw-toxic-comment-classification-challenge/train.csv\")\n",
    "comments_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Separación de datos de entrenamiento y de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34852</th>\n",
       "      <td>This is a straw man argument, Mr Merkey.  Nobody is arguing that federally unverified tribes, clans or groups should be identified as having any kind of federal recognition. What's being said here, something that you seem unwilling to accept as even being a valid view, is that those groups claim to have cherokee lineage (in many cases, having verifiable documented evidence to that effect), and as such, they should be figured on the cherokee page, not simply expunged from wikipedia because they don't fall within the very narrow rules defined by the very government that has historically persecuted them. If nothing else, they should be included to highlight the controversy that their claim to being cherokee engenders.  Legally, they may not be recognised cherokees, but anthropologically they are. Removing non federally recognised groups on the basis of some imagined legal threat would seem to be ridiculous; this is intended, after all, to be an encyclopedia, and thus, encyclopedic.  What it's not intended to be is a vanity publishing service, and to remove all reference to non-recognised tribes and the controversy surrounding them would, in my view at least, turn this article into just that.  It certainly wouldn't, as you imply, improve Wikipedia.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17133</th>\n",
       "      <td>ARC Gritt, the fucking cunt of all cunts, ruined me by saying I vandalised 2009 Formula One season. What a cunt.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           comment_text\n",
       "34852  This is a straw man argument, Mr Merkey.  Nobody is arguing that federally unverified tribes, clans or groups should be identified as having any kind of federal recognition. What's being said here, something that you seem unwilling to accept as even being a valid view, is that those groups claim to have cherokee lineage (in many cases, having verifiable documented evidence to that effect), and as such, they should be figured on the cherokee page, not simply expunged from wikipedia because they don't fall within the very narrow rules defined by the very government that has historically persecuted them. If nothing else, they should be included to highlight the controversy that their claim to being cherokee engenders.  Legally, they may not be recognised cherokees, but anthropologically they are. Removing non federally recognised groups on the basis of some imagined legal threat would seem to be ridiculous; this is intended, after all, to be an encyclopedia, and thus, encyclopedic.  What it's not intended to be is a vanity publishing service, and to remove all reference to non-recognised tribes and the controversy surrounding them would, in my view at least, turn this article into just that.  It certainly wouldn't, as you imply, improve Wikipedia.\n",
       "17133  ARC Gritt, the fucking cunt of all cunts, ruined me by saying I vandalised 2009 Formula One season. What a cunt.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(comments_df[['comment_text']], comments_df['toxic'], random_state=10)\n",
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Normalización o preprocesado del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "GOOD_SYMBOLS = \"€\\?\"\n",
    "GOOD_SYMBOLS_RE = re.compile('([' + GOOD_SYMBOLS + '])')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z '+ GOOD_SYMBOLS + ']')\n",
    "ADD_SPACES_SYMBOLS_RE = re.compile(\"([\\?])\")\n",
    "STEMMER = SnowballStemmer('english')\n",
    "\n",
    "class TextPreprocessor:\n",
    "        \n",
    "    def transfrom_text(self, text):\n",
    "        text = re.sub(GOOD_SYMBOLS_RE, r\"\\1\", text) #process good symbols\n",
    "        text = text.lower()\n",
    "        text = re.sub(REPLACE_BY_SPACE_RE, \" \", text) # process bad symbols\n",
    "        text = re.sub(BAD_SYMBOLS_RE, \"\", text) # process bad symbols\n",
    "        text = re.sub(ADD_SPACES_SYMBOLS_RE, r\" \\1 \", text)\n",
    "        test = \" \".join([STEMMER.stem(word) for word in text.split()])\n",
    "        return text\n",
    "    \n",
    "    def transform(self, series):\n",
    "        return series.apply(lambda text: self.transfrom_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = TextPreprocessor()\n",
    "X_train_preprocessed = preprocessor.transform(X_train['comment_text'])\n",
    "X_test_preprocessed = preprocessor.transform(X_test['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34852    This is a straw man argument, Mr Merkey.  Nobody is arguing that federally unverified tribes, clans or groups should be identified as having any kind of federal recognition. What's being said here, something that you seem unwilling to accept as even being a valid view, is that those groups claim to have cherokee lineage (in many cases, having verifiable documented evidence to that effect), and as such, they should be figured on the cherokee page, not simply expunged from wikipedia because they don't fall within the very narrow rules defined by the very government that has historically persecuted them. If nothing else, they should be included to highlight the controversy that their claim to being cherokee engenders.  Legally, they may not be recognised cherokees, but anthropologically they are. Removing non federally recognised groups on the basis of some imagined legal threat would seem to be ridiculous; this is intended, after all, to be an encyclopedia, and thus, encyclopedic.  What it's not intended to be is a vanity publishing service, and to remove all reference to non-recognised tribes and the controversy surrounding them would, in my view at least, turn this article into just that.  It certainly wouldn't, as you imply, improve Wikipedia.\n",
      "17133    ARC Gritt, the fucking cunt of all cunts, ruined me by saying I vandalised 2009 Formula One season. What a cunt.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
      "Name: comment_text, dtype: object\n",
      "34852    this is a straw man argument  mr merkey  nobody is arguing that federally unverified tribes  clans or groups should be identified as having any kind of federal recognition whats being said here  something that you seem unwilling to accept as even being a valid view  is that those groups claim to have cherokee lineage  in many cases  having verifiable documented evidence to that effect   and as such  they should be figured on the cherokee page  not simply expunged from wikipedia because they dont fall within the very narrow rules defined by the very government that has historically persecuted them if nothing else  they should be included to highlight the controversy that their claim to being cherokee engenders  legally  they may not be recognised cherokees  but anthropologically they are removing non federally recognised groups on the basis of some imagined legal threat would seem to be ridiculous  this is intended  after all  to be an encyclopedia  and thus  encyclopedic  what its not intended to be is a vanity publishing service  and to remove all reference to nonrecognised tribes and the controversy surrounding them would  in my view at least  turn this article into just that  it certainly wouldnt  as you imply  improve wikipedia\n",
      "17133    arc gritt  the fucking cunt of all cunts  ruined me by saying i vandalised 2009 formula one season what a cunt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
      "Name: comment_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_train[\"comment_text\"][:2])\n",
    "print(X_train_preprocessed[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words\n",
    "\n",
    "Ahora vamos a hacer un conteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import   CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train_preprocessed)\n",
    "X_train_vectorized = vectorizer.transform(X_train_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_vectorized = vectorizer.transform(X_test_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(class_weight='balanced')\n",
    "model.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat = model.predict(X_test_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,\\\n",
    "    average_precision_score, roc_auc_score, recall_score\n",
    "\n",
    "def scores(y, predicted):\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y, predicted),\n",
    "        'precision': precision_score(y, predicted),\n",
    "        'recall': recall_score(y, predicted),\n",
    "        'f1-score': f1_score(y, predicted)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.935928609029153,\n",
       " 'precision': 0.6123897199846567,\n",
       " 'recall': 0.8564914163090128,\n",
       " 'f1-score': 0.7141579065086111,\n",
       " 'average-Precision': 0.5379174126909243}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores(y_test, y_test_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompleteModel:\n",
    "    \n",
    "    def __init__(self, preprocessor, vectorizer, model, colname=\"comment_text\"):\n",
    "        self.colname = colname\n",
    "        self.preprocessor = preprocessor\n",
    "        self.vectorizer = vectorizer\n",
    "        self.model = model\n",
    "           \n",
    "    def fit(self, X, y):\n",
    "        print(\"preprocessor...\")\n",
    "        X_fe = pd.DataFrame({self.colname: self.preprocessor.transform(X[self.colname])})\n",
    "        print(\"vectorizer...\")\n",
    "        self.vectorizer.fit(X_fe[self.colname])\n",
    "        print(\"model...\")\n",
    "        X_fe = self.vectorizer.transform(X[self.colname])\n",
    "        self.model.fit(X_fe, y)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X_fe = pd.DataFrame({self.colname: self.preprocessor.transform(X[self.colname])})        \n",
    "        X_fe = self.vectorizer.transform(X_fe[self.colname])\n",
    "        return self.model.predict(X_fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_model = CompleteModel(preprocessor, vectorizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessor...\n",
      "vectorizer...\n",
      "model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.CompleteModel at 0x313ff025f8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9331712330484044,\n",
       " 'precision': 0.5991041433370661,\n",
       " 'recall': 0.8610515021459227,\n",
       " 'f1-score': 0.7065815540391813,\n",
       " 'average-Precision': 0.5288442567261153}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_hat = complete_model.predict(X_test)\n",
    "scores(y_test, y_test_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfIdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=4, max_df=0.9, ngram_range=(1, 2), token_pattern='(\\S+)')\n",
    "complete_tfidf_model = CompleteModel(preprocessor, tfidf_vectorizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessor...\n",
      "vectorizer...\n",
      "model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.CompleteModel at 0x313fef0b38>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_tfidf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9331963001027749,\n",
       " 'precision': 0.6010264208325413,\n",
       " 'recall': 0.848175965665236,\n",
       " 'f1-score': 0.7035265324285237,\n",
       " 'average-Precision': 0.5239641176536308}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_hat = complete_tfidf_model.predict(X_test)\n",
    "scores(y_test, y_test_hat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
